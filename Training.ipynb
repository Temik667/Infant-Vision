{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "759481d2-6ae3-4b38-8064-a225de8ec741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07883813-2311-4483-8870-d812a666abdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T09:34:10.708776Z",
     "iopub.status.busy": "2024-12-30T09:34:10.708511Z",
     "iopub.status.idle": "2024-12-30T09:34:10.713818Z",
     "shell.execute_reply": "2024-12-30T09:34:10.713382Z",
     "shell.execute_reply.started": "2024-12-30T09:34:10.708776Z"
    }
   },
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('/notebooks/tiny-imagenet.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('/notebooks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02f3bf6f-588a-423b-aba0-8741eee5f648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'batch_size': 200, 'name': 'resnet18_color_perception', 'lr': 0.1, 'workers': 4, 'moment': 0.9, 'weight_decay': 0.0001, 'lr_step_size': 30, 'lr_gamma': 0.1, 'total_epochs': 500},\n",
       " 200)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Params:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 200\n",
    "        self.name = \"resnet18_color_perception\"\n",
    "        self.lr = 0.1\n",
    "        self.workers = 4\n",
    "        self.moment = 0.9 ## Deleted bethas added moment\n",
    "        self.weight_decay = 1e-4\n",
    "        self.lr_step_size = 30\n",
    "        self.lr_gamma = 0.1\n",
    "        self.total_epochs = 500\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.__dict__)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.__dict__ == other.__dict__\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "params = Params()\n",
    "params, params.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5b325d9-888c-41da-b2b8-9bb65a374812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "{'n01443537': 0, 'n01629819': 1, 'n01641577': 2, 'n01644900': 3, 'n01698640': 4, 'n01742172': 5, 'n01768244': 6, 'n01770393': 7, 'n01774384': 8, 'n01774750': 9, 'n01784675': 10, 'n01855672': 11, 'n01882714': 12, 'n01910747': 13, 'n01917289': 14, 'n01944390': 15, 'n01945685': 16, 'n01950731': 17, 'n01983481': 18, 'n01984695': 19, 'n02002724': 20, 'n02056570': 21, 'n02058221': 22, 'n02074367': 23, 'n02085620': 24, 'n02094433': 25, 'n02099601': 26, 'n02099712': 27, 'n02106662': 28, 'n02113799': 29, 'n02123045': 30, 'n02123394': 31, 'n02124075': 32, 'n02125311': 33, 'n02129165': 34, 'n02132136': 35, 'n02165456': 36, 'n02190166': 37, 'n02206856': 38, 'n02226429': 39, 'n02231487': 40, 'n02233338': 41, 'n02236044': 42, 'n02268443': 43, 'n02279972': 44, 'n02281406': 45, 'n02321529': 46, 'n02364673': 47, 'n02395406': 48, 'n02403003': 49, 'n02410509': 50, 'n02415577': 51, 'n02423022': 52, 'n02437312': 53, 'n02480495': 54, 'n02481823': 55, 'n02486410': 56, 'n02504458': 57, 'n02509815': 58, 'n02666196': 59, 'n02669723': 60, 'n02699494': 61, 'n02730930': 62, 'n02769748': 63, 'n02788148': 64, 'n02791270': 65, 'n02793495': 66, 'n02795169': 67, 'n02802426': 68, 'n02808440': 69, 'n02814533': 70, 'n02814860': 71, 'n02815834': 72, 'n02823428': 73, 'n02837789': 74, 'n02841315': 75, 'n02843684': 76, 'n02883205': 77, 'n02892201': 78, 'n02906734': 79, 'n02909870': 80, 'n02917067': 81, 'n02927161': 82, 'n02948072': 83, 'n02950826': 84, 'n02963159': 85, 'n02977058': 86, 'n02988304': 87, 'n02999410': 88, 'n03014705': 89, 'n03026506': 90, 'n03042490': 91, 'n03085013': 92, 'n03089624': 93, 'n03100240': 94, 'n03126707': 95, 'n03160309': 96, 'n03179701': 97, 'n03201208': 98, 'n03250847': 99, 'n03255030': 100, 'n03355925': 101, 'n03388043': 102, 'n03393912': 103, 'n03400231': 104, 'n03404251': 105, 'n03424325': 106, 'n03444034': 107, 'n03447447': 108, 'n03544143': 109, 'n03584254': 110, 'n03599486': 111, 'n03617480': 112, 'n03637318': 113, 'n03649909': 114, 'n03662601': 115, 'n03670208': 116, 'n03706229': 117, 'n03733131': 118, 'n03763968': 119, 'n03770439': 120, 'n03796401': 121, 'n03804744': 122, 'n03814639': 123, 'n03837869': 124, 'n03838899': 125, 'n03854065': 126, 'n03891332': 127, 'n03902125': 128, 'n03930313': 129, 'n03937543': 130, 'n03970156': 131, 'n03976657': 132, 'n03977966': 133, 'n03980874': 134, 'n03983396': 135, 'n03992509': 136, 'n04008634': 137, 'n04023962': 138, 'n04067472': 139, 'n04070727': 140, 'n04074963': 141, 'n04099969': 142, 'n04118538': 143, 'n04133789': 144, 'n04146614': 145, 'n04149813': 146, 'n04179913': 147, 'n04251144': 148, 'n04254777': 149, 'n04259630': 150, 'n04265275': 151, 'n04275548': 152, 'n04285008': 153, 'n04311004': 154, 'n04328186': 155, 'n04356056': 156, 'n04366367': 157, 'n04371430': 158, 'n04376876': 159, 'n04398044': 160, 'n04399382': 161, 'n04417672': 162, 'n04456115': 163, 'n04465501': 164, 'n04486054': 165, 'n04487081': 166, 'n04501370': 167, 'n04507155': 168, 'n04532106': 169, 'n04532670': 170, 'n04540053': 171, 'n04560804': 172, 'n04562935': 173, 'n04596742': 174, 'n04597913': 175, 'n06596364': 176, 'n07579787': 177, 'n07583066': 178, 'n07614500': 179, 'n07615774': 180, 'n07695742': 181, 'n07711569': 182, 'n07715103': 183, 'n07720875': 184, 'n07734744': 185, 'n07747607': 186, 'n07749582': 187, 'n07753592': 188, 'n07768694': 189, 'n07871810': 190, 'n07873807': 191, 'n07875152': 192, 'n07920052': 193, 'n09193705': 194, 'n09246464': 195, 'n09256479': 196, 'n09332890': 197, 'n09428293': 198, 'n12267677': 199}\n"
     ]
    }
   ],
   "source": [
    "training_folder = 'tiny-imagenet-200/train'\n",
    "class_to_idx = {}\n",
    "classes = []\n",
    "n_images = 0\n",
    "for folder in os.listdir(training_folder):\n",
    "    if '.' not in folder:\n",
    "        classes.append(folder)\n",
    "        class_folder = \"{}/{}\".format(training_folder, folder)\n",
    "        for sub_folder in os.listdir(class_folder):\n",
    "            if '.' not in sub_folder:\n",
    "                images_folder = \"{}/{}\".format(class_folder, sub_folder)\n",
    "                break\n",
    "        for image in os.listdir(images_folder):\n",
    "            if '.' in image:\n",
    "                n_images += 1\n",
    "classes.sort()\n",
    "for i, class_name in enumerate(classes):\n",
    "    class_to_idx[class_name] = i\n",
    "\n",
    "print(n_images)\n",
    "print(class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80a55a1a-1ef9-4fc8-a0c2-0cd5ed6e94f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Class2Idx:\n",
    "    def __init__(self, new_class_to_idx, original_class_to_idx=None):\n",
    "        self.class_to_idx = new_class_to_idx\n",
    "        self.original_class_to_idx = original_class_to_idx\n",
    "        if self.original_class_to_idx:\n",
    "            self.original_idx_to_class = {v: k for k, v in self.original_class_to_idx.items()}\n",
    "\n",
    "    def __call__(self, target):\n",
    "        if not self.original_class_to_idx:\n",
    "            return self.class_to_idx[target]\n",
    "        return self.class_to_idx[self.original_idx_to_class[target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2f9aabd-d849-48a0-997f-69fe295d7323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "class CustomImageFolder(ImageFolder):\n",
    "    def __init__(self, root, transform=None, custom_class_to_idx=None):\n",
    "        super().__init__(root, transform=transform)\n",
    "\n",
    "        # Override the class_to_idx and classes attributes\n",
    "        if custom_class_to_idx:\n",
    "            self.target_transform = Class2Idx(custom_class_to_idx, original_class_to_idx=self.class_to_idx)\n",
    "            self.class_to_idx = custom_class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0d874ad-699c-4e6e-9b7d-e7e8d7b2c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data = pd.read_csv('tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header= None)\n",
    "# val_data[[0, 1]]\n",
    "# counter = 0\n",
    "# for path, name in zip(val_data[0], val_data[1]):\n",
    "#     image_path = f'tiny-imagenet-200/val/images/{path}'\n",
    "#     name = f'tiny-imagenet-200/val/images/{name}.jpg'\n",
    "#     os.rename(image_path, name)\n",
    "#     counter += 1\n",
    "#     if (counter/100) == 0:\n",
    "#         print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96c9d4ed-fce0-4277-9401-b40a38a48baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_directory = \"/notebooks/tiny-imagenet-200/val/images\"\n",
    "\n",
    "# try:\n",
    "#     # Get a list of all files in the directory\n",
    "#     files = [f for f in os.listdir(source_directory) if os.path.isfile(os.path.join(source_directory, f))]\n",
    "\n",
    "#     # Iterate through the files\n",
    "#     for file_name in files:\n",
    "#         # Create a folder name based on the file name (excluding the extension)\n",
    "#         folder_name = os.path.splitext(file_name)[0]\n",
    "#         folder_path = os.path.join(source_directory, folder_name)\n",
    "        \n",
    "#         # Create the folder\n",
    "#         os.makedirs(folder_path, exist_ok=True)\n",
    "        \n",
    "#         # Move the file into the folder\n",
    "#         source_file_path = os.path.join(source_directory, file_name)\n",
    "#         destination_file_path = os.path.join(folder_path, file_name)\n",
    "#         shutil.move(source_file_path, destination_file_path)\n",
    "        \n",
    "#         print(f\"Moved {file_name} to {folder_path}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54c4302b-4b09-4032-b899-09996813cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, label):\n",
    "    image = image.permute(1, 2, 0)\n",
    "    plt.imshow(image.squeeze())\n",
    "    plt.title(f'Label: {label}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52ea7e73-38b8-46ee-9f19-5788715d14a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "## to get paths and names of each image\n",
    "\n",
    "def images_get_paths(path):\n",
    "    paths = {}\n",
    "    names = {}\n",
    "    rel_paths = ''\n",
    "    for file in os.listdir(path):\n",
    "        if '.' not in file:\n",
    "            for img in os.listdir(path + '/' + file):\n",
    "                rel_paths = path + '/' + file + '/' + os.path.relpath(img)\n",
    "                if file not in paths.keys():\n",
    "                    paths[file] = [rel_paths]\n",
    "                    names[file] = [os.path.relpath(img)]\n",
    "                else:\n",
    "                    paths[file].append(rel_paths)\n",
    "                    names[file].append(os.path.relpath(img))\n",
    "    return paths, names\n",
    "\n",
    "## to organizes files from the given root path into a new structured directory.\n",
    "\n",
    "def organize_files(path):\n",
    "    new_root = os.path.join(\"Dataset\", os.path.basename(path) + \"_organized\")\n",
    "    os.makedirs(new_root, mode=0o777, exist_ok=True)\n",
    "    files_paths, files_names = images_get_paths(path)\n",
    "\n",
    "    for month in files_paths.keys():\n",
    "        month_folder = os.path.join(new_root, month)\n",
    "        os.makedirs(month_folder, mode=0o777, exist_ok=True)\n",
    "        \n",
    "        for name, file_path in zip(files_names[month], files_paths[month]):\n",
    "            category = name.split(\"_\")[0]\n",
    "            category_folder = os.path.join(month_folder, category)\n",
    "            os.makedirs(category_folder, mode=0o777, exist_ok=True)\n",
    "            shutil.move(file_path, os.path.join(category_folder, name))\n",
    "            print(f\"Moved: {file_path} -> {category_folder}\")\n",
    "# organize_files('tiny-imagenet-200/val_reorganized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c845a00-76dc-4452-84ab-a81b107d648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loader for different datasets\n",
    "\n",
    "## I deleted the convertion from BGR to RGB cause I redid dataset\n",
    "def Loader_train(root_folder):\n",
    "    train_transformation = transforms.Compose([\n",
    "            # transforms.Resize((64,64)),\n",
    "            transforms.RandomVerticalFlip(0.5),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.RandomRotation(degrees=(180)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.485, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root = root_folder,\n",
    "        transform = train_transformation\n",
    "    )\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers = params.workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adc41225-eb30-4c48-a87d-a7dfaf71b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transformation = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "        # Normalize the pixel values (in R, G, and B channels)\n",
    "        transforms.Normalize(mean=[0.485, 0.485, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "# val_dataset = ValidationDataset('/tiny-imagenet-200/val/val_annotations.txt','/tiny-imagenet-200/val/images', transform=val_transformation, target_transform=Class2Idx(class_to_idx))\n",
    "val_dataset = CustomImageFolder(\n",
    "        root = \"val_organized\",\n",
    "        transform = val_transformation,\n",
    "        custom_class_to_idx=class_to_idx,\n",
    "    )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=params.workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1510cf1-cf92-46dd-8bfd-6eb25b63ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def train(dataloader, model, loss_fn, optimizer, epoch, writer, dataset_id):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    start0 = time.time()\n",
    "    start = time.time()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_size = len(X)\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * batch_size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}], {(current/size * 100):>4f}%\")\n",
    "            step=epoch+current/size\n",
    "            writer.add_scalar('training loss/dataset_{}'.format(dataset_id), \n",
    "                            loss,\n",
    "                            step)\n",
    "            new_start = time.time()\n",
    "            delta = new_start - start\n",
    "            start = new_start\n",
    "            if batch != 0:\n",
    "                print(\"Done in \", delta, \" seconds\")\n",
    "                remaining_steps = size - current\n",
    "                speed = 100 * batch_size / delta\n",
    "                remaining_time = remaining_steps / speed\n",
    "                print(\"Remaining time (seconds): \", remaining_time)\n",
    "        optimizer.zero_grad()\n",
    "    print(\"Entire epoch done in \", time.time() - start0, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "017ef53d-f806-485e-aadb-6977a8cdeb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, epoch, writer, train_dataloader, dataset_id, calc_acc5=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct, correct_top5 = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            if calc_acc5:\n",
    "                _, pred_top5 = pred.topk(5, 1, largest=True, sorted=True)\n",
    "                correct_top5 += pred_top5.eq(y.view(-1, 1).expand_as(pred_top5)).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    step = epoch\n",
    "    if writer != None:\n",
    "        writer.add_scalar('test loss/dataset_{}'.format(dataset_id),\n",
    "                            test_loss,\n",
    "                            step)\n",
    "    correct /= size\n",
    "    correct_top5 /= size\n",
    "    if writer != None:\n",
    "        writer.add_scalar('test accuracy/dataset_{}'.format(dataset_id),\n",
    "                            100*correct,\n",
    "                            step)\n",
    "        if calc_acc5:\n",
    "            writer.add_scalar('test accuracy5/dataset_{}'.format(dataset_id),\n",
    "                            100*correct_top5,\n",
    "                            step)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    if calc_acc5:\n",
    "        print(f\"Test Error: \\n Accuracy-5: {(100*correct_top5):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b389deb7-5b1e-409f-be96-ecb7ad7da3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18()\n",
    "model.fc=nn.Linear(model.fc.in_features, 200) ## Added this just to make sure that the output fits the labels\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "## Changed the optimizer her for SGD with momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = params.lr, momentum=params.moment, weight_decay = params.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=params.lr_step_size, gamma=params.lr_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2fdb26e-9dfb-40a9-8d49-54ddffe724aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "resume_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab8c4720-7b9e-4931-b93f-154297b1415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "\n",
    "start_dataset_idx = 1\n",
    "start_epoch = 1\n",
    "early_stopping_patience = 10\n",
    "no_improvement_count = 0\n",
    "best_val_accuracy = float('-inf')\n",
    "\n",
    "checkpoint_path = os.path.join(\"checkpoints\", params.name, f\"checkpoint.pth\")\n",
    "\n",
    "if resume_training and os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    start_dataset_idx = checkpoint[\"dataset_idx\"]\n",
    "    best_val_accuracy = checkpoint.get(\"best_val_accuracy\", float('-inf')) \n",
    "    no_improvement_count = checkpoint.get(\"no_improvement_count\", 0)\n",
    "    assert params == checkpoint[\"params\"]\n",
    "\n",
    "Path(os.path.join(\"checkpoints\", params.name)).mkdir(parents=True, exist_ok=True)\n",
    "writer = SummaryWriter('runs/' + params.name)\n",
    "\n",
    "dataset_root = 'Dataset/Color_organized'\n",
    "dataset_folders = [os.path.join(dataset_root, f\"Color_{i}_months\") for i in range(0, 13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bda248c7-08ad-4c28-8029-ba0fcaf9161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "print(start_epoch, start_dataset_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab7983-f008-4cc3-b94b-459d708514ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on dataset 1 at Dataset/Color_organized/Color_0_months\n",
      "loss: 5.363074  [  200/100000], 0.200000%\n",
      "loss: 5.141621  [20200/100000], 20.200000%\n",
      "Done in  243.4505491256714  seconds\n",
      "Remaining time (seconds):  971.3676910114287\n",
      "loss: 4.934868  [40200/100000], 40.200000%\n",
      "Done in  243.27603673934937  seconds\n",
      "Remaining time (seconds):  727.3953498506546\n",
      "loss: 4.925238  [60200/100000], 60.200000%\n",
      "Done in  246.60124802589417  seconds\n",
      "Remaining time (seconds):  490.7364835715294\n",
      "loss: 4.825046  [80200/100000], 80.200000%\n",
      "Done in  242.1637098789215  seconds\n",
      "Remaining time (seconds):  239.7420727801323\n",
      "Entire epoch done in  1246.426864862442  seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/sarbupintemirlan/miniforge3/envs/CVP/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/sarbupintemirlan/miniforge3/envs/CVP/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'CustomImageFolder' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "for dataset_idx, dataset_folder in enumerate(dataset_folders, start=1):\n",
    "    if dataset_idx < start_dataset_idx:\n",
    "        continue\n",
    "\n",
    "    print(f\"Training on dataset {dataset_idx} at {dataset_folder}\")\n",
    "    train_loader = Loader_train(dataset_folder)\n",
    "\n",
    "    for epoch in range(start_epoch if dataset_idx == start_dataset_idx else 1, params.total_epochs):\n",
    "        train(train_loader, model, loss_fn, optimizer, epoch, writer, dataset_idx)\n",
    "        \n",
    "        val_accuracy = test(val_loader, model, loss_fn, epoch, writer, train_dataloader=train_loader, dataset_id=dataset_idx, calc_acc5=True)\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            no_improvement_count = 0\n",
    "            \n",
    "            checkpoint = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"dataset_idx\": dataset_idx,\n",
    "            \"params\": params,\n",
    "            \"best_val_accuracy\": best_val_accuracy,\n",
    "            \"no_improvement_count\": no_improvement_count,\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"Checkpoint successfully saved at {checkpoint_path}\")\n",
    "            print(f\"New best validation accuracy: {val_accuracy:.4f}\")\n",
    "            \n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"No improvement for {no_improvement_count} epochs.\")\n",
    "\n",
    "        if no_improvement_count >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs with no improvement.\")\n",
    "            break\n",
    "    \n",
    "    \n",
    "    start_epoch = 1  \n",
    "    no_improvement_count = 0\n",
    "    print(f\"Finished training on dataset {dataset_idx}.\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e401498-8c52-4144-98e4-ba45b0b74ab6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T12:41:43.456070Z",
     "iopub.status.busy": "2024-12-29T12:41:43.455690Z",
     "iopub.status.idle": "2024-12-29T12:41:59.193766Z",
     "shell.execute_reply": "2024-12-29T12:41:59.192802Z",
     "shell.execute_reply.started": "2024-12-29T12:41:43.456035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 0.2%, Avg loss: 13.236645 \n",
      "\n",
      "Test Error: \n",
      " Accuracy-5: 1.1%, Avg loss: 13.236645 \n",
      "\n",
      "0.0019\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = test(val_loader, model, loss_fn, 1, writer, train_dataloader=train_loader,dataset_id = dataset_idx, calc_acc5=True)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a54bb-20a0-451a-adc8-4df14f828e67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T07:29:09.727613Z",
     "iopub.status.busy": "2024-12-27T07:29:09.726876Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.909402  [   64/100000], 0.064000%\n",
      "loss: 5.078261  [ 6464/100000], 6.464000%\n",
      "Done in  9.46090054512024  seconds\n",
      "Remaining time (seconds):  138.2710614669323\n",
      "loss: 4.922323  [12864/100000], 12.864000%\n",
      "Done in  6.781798362731934  seconds\n",
      "Remaining time (seconds):  92.33418470859527\n",
      "loss: 4.507597  [19264/100000], 19.264000%\n",
      "Done in  7.1010520458221436  seconds\n",
      "Remaining time (seconds):  89.57977155804633\n",
      "loss: 4.929873  [25664/100000], 25.664000%\n",
      "Done in  7.03376317024231  seconds\n",
      "Remaining time (seconds):  81.69715922236443\n",
      "loss: 4.833933  [32064/100000], 32.064000%\n",
      "Done in  6.653949499130249  seconds\n",
      "Remaining time (seconds):  70.63167393326759\n",
      "loss: 4.751972  [38464/100000], 38.464000%\n",
      "Done in  6.292416334152222  seconds\n",
      "Remaining time (seconds):  60.50158305287361\n",
      "loss: 4.657359  [44864/100000], 44.864000%\n",
      "Done in  7.519590616226196  seconds\n",
      "Remaining time (seconds):  64.78127315878868\n"
     ]
    }
   ],
   "source": [
    "# ## Standard Training\n",
    "# train_loader = Loader_train(dataset_root)\n",
    "# for epoch in range(start_epoch, params.total_epochs+1):\n",
    "#     train(train_loader, model, loss_fn, optimizer, epoch=epoch, writer=writer)\n",
    "#     checkpoint = {\n",
    "#         \"model\": model.state_dict(),\n",
    "#         \"optimizer\": optimizer.state_dict(),\n",
    "#         \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "#         \"epoch\": epoch,\n",
    "#         \"params\": params\n",
    "#      }\n",
    "#     torch.save(checkpoint, checkpoint_path)\n",
    "#     lr_scheduler.step()\n",
    "#     test(val_loader, model, loss_fn, epoch + 1, writer, train_dataloader=train_loader, calc_acc5=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
